						Deploying OpenShift 3.11 Cluster 
						################################


Infrastructure Setup: 
*********************

Hostname			IP Address	CPUs	RAM	HDD			OS		Role
master1.faranda.io		172.31.31.118	 4	4 GB	/dev/sda (25 GB)	CentOS7.x	Master Node
infra1.faranda.io		172.31.31.120	 2	2 GB	/dev/sda (25 GB)	CentOS7.x	Infra Node
node1.faranda.io		172.31.31.121	 2	2 GB	/dev/sda (25 GB)	CentOS7.x	Worker Node
node2.faranda.io		172.31.31.122	 2	2 GB	/dev/sda (25 GB)	CentOS7.x	Worker Node
node3.faranda.io		172.31.31.123	 2	2 GB	/dev/sda (25 GB)	CentOS7.x	Worker Node


			
						#################################
						# 	Preparing Nodes: 	#
						#################################

Step 1: Check the hostnames:
***************************

Check the hostname and dns resolution in all nodes:

~]# hostname
~]# hostname -f
~]# cat /etc/resolv.conf 
~]# dig master1.faranda.io
~]# dig +short  master1.faranda.io
~]# dig +short  infra1.faranda.io
~]# dig +short  node1.faranda.io
~]# dig +short  node2.faranda.io
~]# dig +short  node3.faranda.io

~]# dig +short  console.faranda.io
~]# dig +short  apps.faranda.io



Step 2: Disable cloud-init functionality in order to prevent futures network misconfigurations [oVirt]
The issue is described in the following thread:
https://users.ovirt.narkive.com/z4A3RzT5/ovirt-users-cloud-init-reset-network-configuration-to-default-dhcp-after-reboot-and-regular-run#post4

~]# /usr/bin/touch /etc/cloud/cloud-init.disabled


Step 3: Use the below command to update the System on all nodes: 

~]# yum update -y

~]# reboot 

Step 4: Once the systems came back UP/ONLINE, Install the following Packages on all nodes:  

~]# yum install -y wget git  nano net-tools docker-1.13.1 bind-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct openssl-devel httpd-tools NetworkManager python-cryptography python-devel python-passlib java-1.8.0-openjdk-headless "@Development Tools"


Step 5: Configure Ansible Repository on master Node only. 
*********************************************************

cat > /etc/yum.repos.d/ansible.repo << EOF
[ansible]
name = Ansible Repo
baseurl = https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/
enabled = 1
gpgcheck =  0
EOF

Step 6: Start and Enable NetworkManager and Docker Services on all nodes: 
*************************************************************************


~]# systemctl start NetworkManager
~]# systemctl enable NetworkManager
~]# systemctl status NetworkManager

~]# systemctl start docker && systemctl enable docker && systemctl status docker


Step 7: Install Ansible Package and Clone Openshift-Ansible Git Repo on Master Machine :
****************************************************************************************

[root@master ~]# yum -y install ansible-2.8* pyOpenSSL
[root@master ~]# git clone https://github.com/openshift/openshift-ansible.git
[root@master ~]# cd openshift-ansible && git fetch && git checkout release-3.11


Step 8: Generate SSH Keys on Master Node and install it on all nodes: 
*********************************************************************

[root@master ~]# ssh-keygen -f ~/.ssh/id_rsa -N '' 
[root@master ~]# for host in master1.faranda.io node1.faranda.io node2.faranda.io node3.faranda.io infra1.faranda.io ; do ssh-copy-id -i ~/.ssh/id_rsa.pub $host; done

and test

[root@master1 ~]# for host in master1.faranda.io node1.faranda.io node2.faranda.io node3.faranda.io infra1.faranda.io ; do ssh $host hostname; done

Step 9: Now Create Your Own Inventory file for Ansible as following on master Node:  

[root@master ~]# vi ~/inventory.ini

[OSEv3:children]
masters
nodes
etcd

[masters]
master1.faranda.io openshift_ip=172.31.31.118

[etcd]
master1.faranda.io openshift_ip=172.31.31.118

[nodes]
master1.faranda.io openshift_ip=172.31.31.118 openshift_schedulable=true openshift_node_group_name='node-config-master'
node1.faranda.io openshift_ip=172.31.31.121 openshift_schedulable=true openshift_node_group_name='node-config-compute'
node2.faranda.io openshift_ip=172.31.31.122 openshift_schedulable=true openshift_node_group_name='node-config-compute'
node3.faranda.io openshift_ip=172.31.31.123 openshift_schedulable=true openshift_node_group_name='node-config-compute'
infra1.faranda.io openshift_ip=172.31.31.120 openshift_schedulable=true openshift_node_group_name='node-config-infra'

[OSEv3:vars]
debug_level=4
ansible_ssh_user=root
openshift_enable_service_catalog=true
ansible_service_broker_install=true
containerized=false
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
openshift_disable_check=disk_availability,docker_storage,memory_availability,docker_image_availability
#openshift_node_kubelet_args={'pods-per-core': ['10']}
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true'], 'edits': [{ 'key': 'kubeletArguments.pods-per-core','value': ['10']}]}]
deployment_type=origin
openshift_deployment_type=origin

openshift_release=v3.11.0
openshift_pkg_version=-3.11.0
openshift_image_tag=v3.11.0
openshift_service_catalog_image_version=v3.11.0
template_service_broker_image_version=v3.11.0
osm_use_cockpit=true

# put the router on dedicated infra1 node
openshift_hosted_router_selector='region=infra'
openshift_master_default_subdomain=apps.faranda.io
openshift_public_hostname=console.faranda.io

# put the image registry on dedicated infra1 node
openshift_hosted_registry_selector='region=infra'



Step 10: Use the below ansible playbook command to check the prerequisites to deply OpenShift Cluster on master Node: 

[root@master ~]# ansible-playbook -i ~/inventory.ini openshift-ansible/playbooks/prerequisites.yml


Step 11: Once prerequisites completed without any error use the below ansible playbook to Deploy OpenShift Cluster on master Node: 

[root@master ~]# ansible-playbook -i ~/inventory.ini openshift-ansible/playbooks/deploy_cluster.yml


Now you have to wait approx 20-30 Minutes to complete the Installation. 








Step 12: Once the Installation is completed, Create a admin user in OpenShift with Password "Pas$$w0rd" from master Node:  

Install httpd-tools on master

[root@master ~]# htpasswd -c /etc/origin/master/htpasswd admin

New password: Pas$$w0rd
Re-type new password: Pas$$w0rd

[root@master ~]# ls -l /etc/origin/master/htpasswd
[root@master ~]# cat /etc/origin/master/htpasswd


Open /etc/origin/master/master-config.yaml file and do the following changes: 

  identityProviders:
  - challenge: true
    login: true
    mappingMethod: claim
    name: allow_all
    provider:
      apiVersion: v1
      kind: HTPasswdPasswordIdentityProvider		## This line
      file: /etc/origin/master/htpasswd			## This line

	
:wq (save and exit) 


Restart the below services: 

[root@master ~]# systemctl restart origin-master-controllers.service 
[root@master ~]# systemctl restart origin-master-api.service




Step 13: Use the below command to assign cluster-admin Role to admin user: 

[root@master ~]# oc adm policy add-cluster-role-to-user cluster-admin admin


Step 14: Use the below command to login as admin user on CLI: 

[root@master ~]# oc login
Authentication required for https://master.example.com:8443 (openshift)
Username: admin
Password: Pas$$w0rd
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-public
    kube-system
    logging
    management-infra1
    openshift
    openshift-infra1
    openshift-node
    openshift-web-console

Using project "default".


[root@master ~]# oc whoami
admin


Step 15: Use below command to list the projects, pods, nodes, Replication Controllers, Services and Deployment Config. 

[root@master ~]# oc get projects
NAME                    DISPLAY NAME   STATUS
default                                Active
kube-public                            Active
kube-system                            Active
logging                                Active
management-infra1                       Active
openshift                              Active
openshift-infra1                        Active
openshift-node                         Active
openshift-web-console                  Active


[root@master ~]# oc get nodes 
NAME                 STATUS    ROLES     AGE       VERSION
master.example.com   Ready     master    29m       v1.9.1+a0ce1bc657
node1.example.com    Ready     compute   22m       v1.9.1+a0ce1bc657
node2.example.com    Ready     compute   22m       v1.9.1+a0ce1bc657
node3.example.com    Ready     <none>    22m       v1.9.1+a0ce1bc657


[root@master ~]# oc get pod --all-namespaces
NAMESPACE               NAME                          READY     STATUS    RESTARTS   AGE
default                 docker-registry-1-krmpx       1/1       Running   0          15m
default                 registry-console-1-z9sbd      1/1       Running   0          14m
default                 router-1-q7g5v                1/1       Running   0          15m
openshift-web-console   webconsole-5f649b49b5-hr9dq   1/1       Running   0          14m


[root@master ~]# oc get rc 
NAME                 DESIRED   CURRENT   READY     AGE
docker-registry-1    1         1         1         15m
registry-console-1   1         1         1         14m
router-1             1         1         1         16m


[root@master ~]# oc get svc 
NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE
docker-registry    ClusterIP   172.30.34.127    <none>        5000/TCP                  16m
kubernetes         ClusterIP   172.30.0.1       <none>        443/TCP,53/UDP,53/TCP     35m
registry-console   ClusterIP   172.30.62.197    <none>        9000/TCP                  15m
router             ClusterIP   172.30.141.143   <none>        80/TCP,443/TCP,1936/TCP   16m


[root@master ~]# oc get dc
NAME               REVISION   DESIRED   CURRENT   TRIGGERED BY
docker-registry    1          1         1         config
registry-console   1          1         1         config
router             1          1         1         config



Verifying Multiple etcd Hosts:
##############################

On a master host, verify the etcd cluster health, substituting for the FQDNs of your etcd hosts in
the following:

[root@master ~]# yum install etcd -y 

[root@master ~]# etcdctl -C https://master.openshift.example.com:2379 --ca-file=/etc/origin/master/master.etcd-ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key cluster-health

member b211b9f9d8f23828 is healthy: got healthy result from https://192.168.56.10:2379
cluster is healthy


Also verify the member list is correct:

[root@master ~]# etcdctl -C https://master.openshift.example.com:2379 --ca-file=/etc/origin/master/master.etcd-ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key member list

b211b9f9d8f23828: name=master.example.com peerURLs=https://192.168.56.10:2380 clientURLs=https://192.168.56.10:2379 isLeader=true


Now you can access OpenShift Cluster using Web Browser as following: 


https://master.openshift.example.com:8443 

Username: admin
Password: Pas$$w0rd


Step 17: Now Create a Demo Project 

Click on "Create Project"

Name: demo

Display Name: demo project

Description: This is Demo Project

Click on Create. 


Now Deploy an application in demo project for testing: 

Click in demo project -> Click on Brows Catalog -> Select PHP -> Next

Version : 7.0 - latest 

Application Name: demoapp1

Git Repo: https://github.com/sureshchandrarhca15/OpenShift39.git


Next -> Finish. 


Now click on Overview in demo project

See, demoapp1 build is running. so wait for build completion. 

Once build completed, there is a Pod Running for testapp and will have an URL as below: 

http://demoapp1-demo.apps.openshift.example.com 

Now click on Pod Icon to get the details. 


NOTE: I don't have DNS to resolve App URL. So, I am using /etc/hosts file to resolve it using infra1 node IP Address. 
	because on infra1 node our router pod is running so all traffic will routed from infra1 node in OpenShift Cluster. 


~]# vim /etc/hosts

#Add the below line 

192.168.0.53	demoapp1-demo.apps.openshift.example.com

:wq (save and exit) 


Now Click on Pod URL Link and application should be accessible. 

NOTE: You have to configure /etc/hosts file on that system from where you are accessing the Openshift Dashboard. 
